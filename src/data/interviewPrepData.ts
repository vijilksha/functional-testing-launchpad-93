// Complete Interview Preparation Package with all questions from all tabs
// Enhanced with STLC Agile and Agile & Jira examples and sample documents

export interface InterviewCategory {
  category: string;
  icon: string;
  description: string;
  questions: EnhancedQuestion[];
}

export interface FollowUpQuestion {
  question: string;
  answer: string;
  realTimeExample: string;
}

export interface EnhancedQuestion {
  id: string;
  question: string;
  shortAnswer: string;
  detailedAnswer: string;
  stlcAgileExample?: {
    context: string;
    realWorldScenario: string;
    sampleDocument?: string;
  };
  agileJiraExample?: {
    context: string;
    realWorldScenario: string;
    sampleDocument?: string;
  };
  tips: string[];
  commonMistakes: string[];
  followUpQuestions: (string | FollowUpQuestion)[];
  difficulty: "Beginner" | "Intermediate" | "Advanced";
}

export const interviewCategories: InterviewCategory[] = [
  {
    category: "SDLC & STLC Fundamentals",
    icon: "ğŸ”„",
    description: "Core questions about Software Development and Testing Life Cycles",
    questions: [
      {
        id: "STLC-001",
        question: "What is STLC and how does it differ from SDLC?",
        shortAnswer: "STLC is Software Testing Life Cycle - phases for testing software. SDLC is broader, covering entire software development.",
        detailedAnswer: `STLC (Software Testing Life Cycle) consists of specific phases:
1. Requirement Analysis - Understand what to test
2. Test Planning - Define strategy, resources, timeline
3. Test Case Development - Write detailed test cases
4. Environment Setup - Prepare test environment
5. Test Execution - Run tests, log defects
6. Test Cycle Closure - Final reports, lessons learned

SDLC (Software Development Life Cycle) includes:
Requirements â†’ Design â†’ Development â†’ Testing â†’ Deployment â†’ Maintenance

Key Difference: STLC is a subset focusing only on testing activities within SDLC.`,
        stlcAgileExample: {
          context: "In an Agile STLC approach, these phases happen within each sprint rather than sequentially.",
          realWorldScenario: "For an E-Commerce checkout feature, STLC phases occur during the 2-week sprint: Day 1-2 (Requirement Analysis), Day 3 (Test Planning), Day 4-6 (Test Case Development), Day 7-12 (Execution), Day 13-14 (Closure).",
          sampleDocument: `SPRINT STLC MAPPING - E-Commerce Checkout
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Sprint: Sprint 5 | Duration: 2 weeks
Feature: Shopping Cart Checkout

PHASE TIMELINE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Day 1-2   â”‚ Requirement Analysis        â”‚
â”‚           â”‚ - Review user stories       â”‚
â”‚           â”‚ - Clarify acceptance criteriaâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Day 3     â”‚ Test Planning               â”‚
â”‚           â”‚ - Identify test scope       â”‚
â”‚           â”‚ - Allocate resources        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Day 4-6   â”‚ Test Case Development       â”‚
â”‚           â”‚ - Write test cases          â”‚
â”‚           â”‚ - Prepare test data         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Day 7-12  â”‚ Test Execution              â”‚
â”‚           â”‚ - Execute tests             â”‚
â”‚           â”‚ - Log and verify defects    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Day 13-14 â”‚ Test Closure                â”‚
â”‚           â”‚ - Prepare metrics report    â”‚
â”‚           â”‚ - Sprint retrospective      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "In Jira, STLC phases are tracked using custom workflows and issue types.",
          realWorldScenario: "Create a Test Plan issue linked to the Epic. Test Cases are sub-tasks. Use Zephyr or Xray for test management integrated with Jira.",
          sampleDocument: `JIRA STLC TRACKING SETUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Issue Types:
â”œâ”€â”€ Epic: User Management
â”‚   â”œâ”€â”€ Story: US-101 User Registration
â”‚   â”‚   â”œâ”€â”€ Test Plan: TP-101
â”‚   â”‚   â”œâ”€â”€ Test Case: TC-101-01
â”‚   â”‚   â”œâ”€â”€ Test Case: TC-101-02
â”‚   â”‚   â””â”€â”€ Bug: BUG-201 (linked)
â”‚   â””â”€â”€ Story: US-102 User Login
â”‚       â”œâ”€â”€ Test Plan: TP-102
â”‚       â””â”€â”€ Test Cases...

Custom Fields:
- STLC Phase: Analysis | Planning | Development | Execution | Closure
- Test Status: Not Started | In Progress | Passed | Failed | Blocked`
        },
        tips: [
          "Always relate STLC phases to specific activities in your projects",
          "Mention how phases overlap in Agile vs Waterfall",
          "Be prepared to explain entry/exit criteria for each phase"
        ],
        commonMistakes: [
          "Confusing STLC with SDLC",
          "Not mentioning all 6 phases",
          "Describing phases without real examples"
        ],
        followUpQuestions: [
          {
            question: "What are entry and exit criteria for Test Execution phase?",
            answer: `**Entry Criteria for Test Execution:**
- Test Plan approved
- Test cases reviewed and approved
- Test environment ready with correct build deployed
- Test data prepared and validated
- Required access/permissions granted

**Exit Criteria for Test Execution:**
- All planned test cases executed (95%+ execution rate)
- All critical and high priority defects resolved and retested
- Test summary report generated
- Defect metrics within acceptable threshold
- Sign-off from QA Lead/Manager`,
            realTimeExample: `In our E-Commerce project, Sprint 8 Payment Gateway testing had these criteria:

**Entry Criteria (All met before starting):**
âœ… Build 2.3.5 deployed to QA environment
âœ… 45 test cases approved in Zephyr
âœ… Stripe sandbox API credentials configured
âœ… Test accounts created with various card types

**Exit Criteria (Checked before sign-off):**
âœ… 43/45 test cases executed (96%)
âœ… 2 cases blocked (third-party API issue - documented)
âœ… 0 Critical bugs open, 1 Medium deferred to next sprint
âœ… Test execution report shared with stakeholders`
          },
          {
            question: "How do you handle STLC in Agile sprints?",
            answer: `In Agile, STLC phases run within each sprint rather than sequentially across the project:

**Sprint STLC Mapping:**
1. **Requirement Analysis** - During Sprint Planning and Backlog Refinement
2. **Test Planning** - First 1-2 days of sprint, lightweight mini test plans
3. **Test Case Development** - Parallel with development, based on acceptance criteria
4. **Environment Setup** - Continuous, often automated via CI/CD
5. **Test Execution** - Continuous testing as features complete
6. **Closure** - Sprint Review and Retrospective

**Key Differences from Waterfall:**
- Phases overlap significantly
- Testing starts from Day 1
- Testers collaborate with developers throughout
- Automation is integrated from the start`,
            realTimeExample: `Our Banking app sprint for "Account Statement Download":

**Day 1-2 (Planning + Analysis):**
- Attended Sprint Planning, clarified 3 acceptance criteria
- Created 2-page mini test plan covering scope and risks

**Day 3-6 (Test Development + Environment):**
- Wrote 25 test cases while dev was coding
- Automated 10 smoke tests using existing framework
- Verified statement format templates in test environment

**Day 7-12 (Continuous Execution):**
- Started testing each completed sub-feature
- Found 4 bugs - 3 fixed same day, 1 medium priority

**Day 13-14 (Closure):**
- Final regression run (automated)
- Demo in Sprint Review with test metrics
- Retro action: "Improve PDF generation test data"`
          },
          {
            question: "What happens if requirements change during Test Case Development?",
            answer: `Requirements changes in the middle of testing are common, especially in Agile. Here's how to handle them:

**Immediate Steps:**
1. Assess the impact on existing test cases
2. Communicate with team about timeline implications
3. Update/create new test cases for changed requirements
4. Review test data validity
5. Adjust test plan scope if needed

**Best Practices:**
- Maintain requirement traceability matrix
- Version control test cases
- Use modular test case design for easier updates
- Flag impacted test cases immediately
- Document change in test summary report

**Impact Analysis Questions:**
- Which test cases are affected?
- Do we need new test data?
- Is environment still valid?
- Timeline impact?`,
            realTimeExample: `Insurance Claims project - Mid-sprint requirement change:

**Original Requirement:** Auto-approve claims under $500
**Changed To:** Auto-approve claims under $300, with manager review for $300-$500

**Our Response:**
1. **Impact Analysis (2 hours):**
   - 8 of 15 test cases affected
   - Test data: 12 claim amounts needed adjustment
   - New test cases needed: 5 (for manager review flow)

2. **Actions Taken:**
   - Updated 8 test cases (marked v2 in test management tool)
   - Created 5 new test cases for manager approval workflow
   - Refreshed test data with new boundary values ($299, $300, $301, $499, $500)
   - Sprint extended by 1 day with stakeholder approval

3. **Documentation:**
   - Logged change in Jira under Epic
   - Updated test plan "Scope Changes" section
   - Added note in test summary report about mid-sprint change`
          }
        ],
        difficulty: "Beginner"
      },
      {
        id: "STLC-002",
        question: "What is a Test Plan and what are its key components?",
        shortAnswer: "A Test Plan is a document outlining testing strategy, scope, approach, resources, and schedule for a project.",
        detailedAnswer: `Key components of a Test Plan:

1. **Test Plan ID** - Unique identifier
2. **Introduction** - Purpose and objectives
3. **Scope** - What's in/out of testing scope
4. **Test Items** - Features to be tested
5. **Approach/Strategy** - How testing will be performed
6. **Entry/Exit Criteria** - Start and end conditions
7. **Test Deliverables** - Documents to be produced
8. **Resources** - Team, tools, environments needed
9. **Schedule** - Timeline and milestones
10. **Risks & Mitigation** - Potential issues and solutions
11. **Approvals** - Sign-off requirements`,
        stlcAgileExample: {
          context: "In Agile, Test Plans are lightweight and created per user story or sprint rather than entire project.",
          realWorldScenario: "For a Banking Fund Transfer feature, a mini test plan covers that specific functionality within the sprint.",
          sampleDocument: `AGILE TEST PLAN - Fund Transfer Feature
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
User Story: US-301 Bank Transfer
Sprint: Sprint 8

OBJECTIVE:
Validate fund transfer between accounts works correctly 
with proper validation and error handling.

SCOPE:
âœ“ In Scope:
  - Same bank transfers
  - Inter-bank NEFT/RTGS transfers
  - Transfer validation rules
  - Transaction history update

âœ— Out of Scope:
  - International transfers
  - Scheduled transfers (future sprint)

TEST APPROACH:
- Functional testing: Positive & negative scenarios
- Security testing: Authentication, encryption
- Integration testing: Core banking API

ENTRY CRITERIA:
â–¡ User story in "Ready for Testing" status
â–¡ Test environment deployed with latest build
â–¡ Test data prepared (test accounts)
â–¡ API endpoints available

EXIT CRITERIA:
â–¡ All P1 test cases executed
â–¡ No critical or high severity bugs open
â–¡ Test coverage â‰¥ 90%
â–¡ Sign-off from Product Owner

RISKS:
| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| API instability | Medium | High | Mock services |
| Test data issues | Low | Medium | Data refresh script |`
        },
        agileJiraExample: {
          context: "Test Plans can be created as Confluence pages linked to Jira Epics/Stories, or using test management tools.",
          realWorldScenario: "Create a Test Plan in Confluence, link it to the Epic in Jira. Use Jira queries to track test case status.",
          sampleDocument: `JIRA + CONFLUENCE TEST PLAN SETUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONFLUENCE PAGE STRUCTURE:
ğŸ“„ Test Plan - Sprint 8 Fund Transfer
â”œâ”€â”€ Linked Epic: BANK-EP-005
â”œâ”€â”€ Test Scope
â”œâ”€â”€ Test Cases (embedded Jira filter)
â”œâ”€â”€ Entry/Exit Criteria
â””â”€â”€ Sign-off Section

JIRA FILTER FOR TEST TRACKING:
JQL: project = BANK AND type = "Test Case" 
     AND "Epic Link" = BANK-EP-005
     
JIRA DASHBOARD GADGETS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Execution Status    â”‚ Defects Open â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% Executed â”‚ ğŸ”´ 3 Critical â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 75% Passed   â”‚ ğŸŸ¡ 5 Major    â”‚
â”‚ â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20% Failed   â”‚ ğŸŸ¢ 8 Minor    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Keep Agile test plans concise - 1-2 pages max",
          "Focus on what's unique for this feature, not generic content",
          "Always include entry/exit criteria - interviewers love this"
        ],
        commonMistakes: [
          "Creating overly detailed test plans in Agile",
          "Not including risk assessment",
          "Missing entry/exit criteria"
        ],
        followUpQuestions: [
          {
            question: "How detailed should a test plan be in Agile?",
            answer: `In Agile, test plans should be lightweight and focused - typically 1-2 pages maximum per sprint or feature.

**Key Characteristics:**
- Focus on sprint-specific scope, not generic content
- Include only what's unique to this feature/sprint
- Skip sections that haven't changed from the strategy
- Update iteratively as the sprint progresses

**Essential Sections (Keep):**
- Scope (In/Out)
- Entry/Exit Criteria
- Key Risks
- Test Data needs

**Optional Sections (Skip if unchanged):**
- Team roles (defined at project level)
- Tool details (in Test Strategy)
- Standard processes`,
            realTimeExample: `For our E-Commerce "Wishlist" feature sprint:

**Lightweight Test Plan (1 page):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPRINT 10 - WISHLIST FEATURE TEST PLAN  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stories: SHOP-201, SHOP-202, SHOP-203   â”‚
â”‚                                         â”‚
â”‚ IN SCOPE:                               â”‚
â”‚ âœ“ Add to wishlist from PDP              â”‚
â”‚ âœ“ View wishlist page                    â”‚
â”‚ âœ“ Remove from wishlist                  â”‚
â”‚ âœ“ Move to cart                          â”‚
â”‚                                         â”‚
â”‚ OUT OF SCOPE:                           â”‚
â”‚ âœ— Share wishlist (Sprint 11)            â”‚
â”‚ âœ— Multiple wishlists (backlog)          â”‚
â”‚                                         â”‚
â”‚ ENTRY: Stories in "Ready for Test"      â”‚
â”‚ EXIT: 0 Critical bugs, 90% executed     â”‚
â”‚                                         â”‚
â”‚ RISKS: Third-party cart API latency     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 1 page vs. 10+ pages in Waterfall!`
          },
          {
            question: "Who approves the test plan?",
            answer: `Test Plan approval depends on the organization structure and project methodology:

**In Agile:**
- Test Lead or QA Lead reviews and approves
- Product Owner validates scope alignment
- Scrum Master ensures it fits sprint goals
- Often informal - discussed in Sprint Planning

**In Traditional/Waterfall:**
- QA Manager formally approves
- Project Manager signs off
- Client/Stakeholder approval for large projects
- Change Control Board for modifications

**Approval Levels:**
- Sprint Test Plan: QA Lead + PO (lightweight)
- Release Test Plan: QA Manager + PM
- Master Test Strategy: QA Director + PMO`,
            realTimeExample: `Banking Project Test Plan Approval Flow:

**Sprint Level (Agile):**
Day 1 Sprint Planning:
- QA Lead presents 1-page test plan
- PO confirms scope matches sprint goals
- Scrum Master logs in Confluence
- Verbal approval, no formal sign-off

**Release Level:**
Week before release:
- QA Manager reviews consolidated test plan
- Project Manager signs off on timeline
- Compliance Officer verifies security testing included
- Formal approval in Document Management System

**Audit Trail:**
- All approvals tracked in Confluence
- Version history maintained
- Digital signatures for compliance`
          },
          {
            question: "How often do you update the test plan?",
            answer: `Test Plan updates depend on the type and project methodology:

**Sprint Test Plan (Agile):**
- Created fresh each sprint
- Updated if scope changes mid-sprint
- Closed at sprint end with summary

**Release Test Plan:**
- Major update per release cycle
- Minor updates for scope changes
- Reviewed before each testing phase

**Test Strategy:**
- Updated quarterly or annually
- Changed when tools/processes change
- Reviewed for new project types

**When to Update:**
- New requirements added
- Risk profile changes
- Timeline shifts
- Resource changes
- After major incidents`,
            realTimeExample: `Telecom Project Update Schedule:

**Test Strategy:** Updated once per year
- Last updated: January (added AI testing tools section)
- Next review: December

**Release Test Plan:** Updated per release (6 per year)
- Release 2024.3: Updated when payment module added mid-cycle
- Added 2 sections: Payment testing scope, PCI compliance checks

**Sprint Test Plan:** Created/closed each sprint
Sprint 15 Updates:
- Day 1: Initial plan created
- Day 5: Updated when Story TELE-305 descoped
- Day 12: Added risk for API timeout discovered
- Day 14: Closed with test summary, lessons learned

All updates tracked in Confluence with version history:
v1.0 â†’ v1.1 (scope change) â†’ v1.2 (final)`
          }
        ],
        difficulty: "Beginner"
      },
      {
        id: "STLC-003",
        question: "Explain the difference between Test Strategy and Test Plan.",
        shortAnswer: "Test Strategy is organization-wide testing approach. Test Plan is project/feature-specific testing document.",
        detailedAnswer: `**Test Strategy:**
- High-level document
- Created once for organization/product line
- Defines overall testing approach
- Covers tools, environments, standards
- Rarely changes
- Created by Test Manager/Lead

**Test Plan:**
- Detailed document
- Created per project/release/sprint
- Specific scope, timeline, resources
- Derived from Test Strategy
- Updated frequently
- Created by Test Lead/QA

Example: Test Strategy says "Use Selenium for web automation"
Test Plan says "Sprint 5 will automate 15 login test cases using Selenium"`,
        stlcAgileExample: {
          context: "In Agile, Test Strategy is set at program level, Test Plans at sprint/story level.",
          realWorldScenario: "An Insurance company has one Test Strategy covering all products. Each sprint for Claims Processing has its own mini Test Plan.",
          sampleDocument: `STRATEGY vs PLAN COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TEST STRATEGY (Organization Level):
â”œâ”€â”€ Testing Approach: Risk-based testing
â”œâ”€â”€ Tools: Selenium, JMeter, Postman
â”œâ”€â”€ Environments: Dev, QA, Staging, Prod
â”œâ”€â”€ Automation Policy: Automate regression
â”œâ”€â”€ Defect Process: Log in Jira, SLA 24hr
â””â”€â”€ Standards: IEEE 829, ISTQB guidelines

TEST PLAN (Sprint Level):
â”œâ”€â”€ Sprint: 12 - Claims Auto-Approval
â”œâ”€â”€ Scope: Auto-approve claims < $500
â”œâ”€â”€ Test Cases: 45 functional, 10 integration
â”œâ”€â”€ Resources: 2 QA engineers, 5 days
â”œâ”€â”€ Automation: 20 cases (44%)
â””â”€â”€ Timeline: 
    - Day 1-2: Test case review
    - Day 3-7: Execution
    - Day 8: Defect retesting
    - Day 9-10: Sign-off`
        },
        agileJiraExample: {
          context: "Strategy stored in Confluence wiki, Plans as sprint documents or Jira issues.",
          realWorldScenario: "Test Strategy is a Confluence space with all standards. Each sprint's Test Plan is a child page with specific details.",
          sampleDocument: `CONFLUENCE STRUCTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ QA Wiki (Space)
â”œâ”€â”€ ğŸ“„ Test Strategy (Master Document)
â”‚   â”œâ”€â”€ Testing Approach
â”‚   â”œâ”€â”€ Tool Standards
â”‚   â”œâ”€â”€ Environment Strategy
â”‚   â””â”€â”€ Process Guidelines
â”‚
â”œâ”€â”€ ğŸ“ Release 2024-Q2
â”‚   â”œâ”€â”€ ğŸ“„ Test Plan - Sprint 10
â”‚   â”œâ”€â”€ ğŸ“„ Test Plan - Sprint 11
â”‚   â””â”€â”€ ğŸ“„ Test Plan - Sprint 12
â”‚
â””â”€â”€ ğŸ“ Templates
    â”œâ”€â”€ ğŸ“„ Test Plan Template
    â””â”€â”€ ğŸ“„ Test Summary Template`
        },
        tips: [
          "Strategy = What & Why, Plan = How & When",
          "Mention that Strategy is reusable, Plan is specific",
          "Be ready with examples from your experience"
        ],
        commonMistakes: [
          "Using the terms interchangeably",
          "Not understanding hierarchy (Strategy â†’ Plan)",
          "Creating too detailed Strategy or too vague Plan"
        ],
        followUpQuestions: [
          {
            question: "Who creates the Test Strategy?",
            answer: `Test Strategy is created by senior testing leadership:

**Primary Authors:**
- Test Manager / QA Manager
- Test Architect (for large organizations)
- QA Director (for enterprise-level)

**Contributors:**
- Senior Test Leads (domain expertise)
- DevOps (CI/CD and environment strategy)
- Architecture Team (technical constraints)
- Security Team (security testing standards)

**Reviewers & Approvers:**
- Project Management Office (PMO)
- Engineering Director
- CTO (for critical products)

The Test Strategy should be reviewed and updated periodically, typically annually or when major changes occur.`,
            realTimeExample: `Insurance Company Test Strategy Creation:

**Author:** QA Director (Sarah) with 15+ years experience

**Contributors:**
- Test Manager (Banking domain expertise)
- Test Manager (Claims domain expertise)  
- DevOps Lead (CI/CD pipeline standards)
- InfoSec Lead (OWASP testing requirements)

**Review Committee:**
- VP of Engineering
- Product Director
- Compliance Officer

**Timeline:**
- Week 1-2: Draft by QA Director
- Week 3: Review with contributors
- Week 4: Leadership review and approval
- Annual review every December

Document stored in Confluence with restricted edit access.`
          },
          {
            question: "How often is Test Strategy updated?",
            answer: `Test Strategy updates are infrequent compared to Test Plans:

**Regular Updates:**
- Annual review (minimum)
- Quarterly check for relevance

**Trigger-Based Updates:**
- New tools adopted (e.g., switching from Selenium to Playwright)
- Major process changes (moving from Waterfall to Agile)
- New testing types added (AI testing, accessibility)
- Organizational restructuring
- After major production incidents
- New compliance requirements

**What Typically Changes:**
- Tool stack updates
- Environment strategy
- Automation policies
- Security testing standards
- Metrics and reporting requirements`,
            realTimeExample: `E-Commerce Platform Strategy Updates (2024):

**Scheduled Annual Review (January):**
- Added AI-powered testing section (new tools)
- Updated automation target: 60% â†’ 75%
- Revised environment strategy (cloud migration)

**Unscheduled Update (March):**
Trigger: GDPR audit finding
- Added data privacy testing requirements
- Included test data anonymization standards

**Unscheduled Update (August):**
Trigger: Mobile app launch
- Added mobile testing strategy
- Updated device/browser matrix
- Added performance benchmarks for mobile

**Version History:**
v3.0 (Jan 2024) - Annual update
v3.1 (Mar 2024) - GDPR compliance
v3.2 (Aug 2024) - Mobile strategy`
          },
          {
            question: "Can you have a Test Plan without a Test Strategy?",
            answer: `Yes, but it's not ideal. Here's why:

**Without Strategy:**
- Each Test Plan reinvents the wheel
- Inconsistent testing approaches across projects
- No organizational standards
- Higher risk of missing important aspects
- More effort per Test Plan

**With Strategy:**
- Test Plans reference common standards
- Consistent quality across projects
- Faster Test Plan creation
- Clear guidelines for testers
- Reduced redundancy

**When It Happens:**
- Small companies without formal QA process
- Startups in early stages
- One-off projects
- New QA teams still building processes

**Recommendation:**
Even a simple 1-page Test Strategy is better than none. It can grow over time.`,
            realTimeExample: `Startup vs Enterprise Comparison:

**Startup (No Strategy - Initially):**
Project A: "We'll test on Chrome"
Project B: "Let's test on all browsers"
Project C: "What browsers should we test?"

Result: Inconsistent, repeated discussions, wasted time

**After Creating Simple Strategy:**
All Projects: "Per our strategy, test on Chrome, Firefox, Safari. Mobile: iOS Safari, Android Chrome."

Time saved: 2 hours per sprint on browser discussions!

**Enterprise (With Strategy):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEST STRATEGY (Referenced by all plans) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Browser Matrix: Defined               â”‚
â”‚ â€¢ Automation: Selenium Grid             â”‚
â”‚ â€¢ Environments: Dev â†’ QA â†’ Stage â†’ Prod â”‚
â”‚ â€¢ Defect Process: Jira with SLA         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TEST PLAN (Sprint 15):
"Following the Test Strategy v3.2, this sprint will..."
(No need to redefine standards)`
          }
        ],
        difficulty: "Intermediate"
      }
    ]
  },
  {
    category: "Test Case Design & Writing",
    icon: "ğŸ“",
    description: "Questions about writing effective test cases and test design techniques",
    questions: [
      {
        id: "TC-001",
        question: "How do you write effective test cases?",
        shortAnswer: "Effective test cases are clear, concise, have defined preconditions, steps, expected results, and are traceable to requirements.",
        detailedAnswer: `Components of an effective test case:

1. **Test Case ID**: Unique identifier (e.g., TC_LOGIN_001)
2. **Title**: Brief description of what's being tested
3. **Preconditions**: Setup required before execution
4. **Test Steps**: Clear, numbered action steps
5. **Test Data**: Specific values to use
6. **Expected Result**: What should happen
7. **Priority**: P1/P2/P3 for execution order
8. **Type**: Positive/Negative/Boundary/Edge Case

Best Practices:
- One test case = one scenario
- Steps should be reproducible by anyone
- Expected result must be verifiable
- Avoid vague terms like "system works properly"`,
        stlcAgileExample: {
          context: "In Agile STLC, test cases are derived directly from acceptance criteria of user stories.",
          realWorldScenario: "For E-Commerce Add to Cart story, each acceptance criterion becomes one or more test cases.",
          sampleDocument: `TEST CASE DERIVATION FROM USER STORY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USER STORY: EC-US-010 Add to Cart
As a customer, I want to add products to cart
So that I can purchase multiple items

ACCEPTANCE CRITERIA â†’ TEST CASES:

AC1: "Add to cart button visible on product pages"
â”œâ”€â”€ TC_CART_001: Verify Add to Cart on product listing
â””â”€â”€ TC_CART_002: Verify Add to Cart on product detail

AC2: "Quantity can be specified before adding"
â”œâ”€â”€ TC_CART_003: Add item with default quantity (1)
â”œâ”€â”€ TC_CART_004: Add item with quantity 5
â””â”€â”€ TC_CART_005: Add item with quantity 0 (negative)

AC3: "Out of stock items cannot be added"
â”œâ”€â”€ TC_CART_006: Verify Add to Cart disabled for out of stock
â””â”€â”€ TC_CART_007: Verify message shown for out of stock

SAMPLE TEST CASE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TC_CART_004: Add Item with Custom Quantity â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Story: EC-US-010                           â”‚
â”‚ Priority: P1 | Type: Positive              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Preconditions:                             â”‚
â”‚ - User is on product detail page           â”‚
â”‚ - Product has 10+ items in stock           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Steps:                                     â”‚
â”‚ 1. Select quantity = 5                     â”‚
â”‚ 2. Click "Add to Cart" button              â”‚
â”‚ 3. Observe cart icon                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Test Data:                                 â”‚
â”‚ Product: iPhone 15, Qty: 5                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Expected Result:                           â”‚
â”‚ - Success toast "Added to cart"            â”‚
â”‚ - Cart icon shows 5                        â”‚
â”‚ - Cart page shows iPhone 15 x 5            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "Test cases linked to user stories in Jira using test management plugins or sub-tasks.",
          realWorldScenario: "Each test case is a sub-task under the user story, or linked issue using Zephyr/Xray.",
          sampleDocument: `JIRA TEST CASE STRUCTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EPIC: Shopping Cart (SHOP-100)
â””â”€â”€ Story: SHOP-110 Add to Cart
    â”œâ”€â”€ [Test] SHOP-110-TC01: Add single item
    â”œâ”€â”€ [Test] SHOP-110-TC02: Add with quantity
    â”œâ”€â”€ [Test] SHOP-110-TC03: Add out of stock
    â”œâ”€â”€ [Test] SHOP-110-TC04: Cart count update
    â””â”€â”€ [Bug] SHOP-BUG-045: Count not updating

TEST CASE JIRA FIELDS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Issue Type: Test Case                   â”‚
â”‚ Summary: Add item with custom quantity  â”‚
â”‚ Story Link: SHOP-110                    â”‚
â”‚ Test Priority: P1                       â”‚
â”‚ Test Type: Functional                   â”‚
â”‚ Automation Status: To Automate          â”‚
â”‚ Execution Status: Not Executed          â”‚
â”‚                                         â”‚
â”‚ Preconditions:                          â”‚
â”‚ [Text field with setup steps]           â”‚
â”‚                                         â”‚
â”‚ Test Steps: (Using Zephyr format)       â”‚
â”‚ Step 1: Select qty=5                    â”‚
â”‚ Step 2: Click Add to Cart               â”‚
â”‚ Expected: Success message shown         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Always trace test cases to requirements",
          "Include both positive and negative scenarios",
          "Make steps detailed enough for junior testers"
        ],
        commonMistakes: [
          "Writing vague expected results",
          "Missing preconditions",
          "Combining multiple scenarios in one test case"
        ],
        followUpQuestions: [
          {
            question: "How do you handle test case maintenance?",
            answer: `Test case maintenance is critical for keeping your test suite relevant and efficient:

**Regular Maintenance Activities:**
- Review test cases after each release
- Update steps when UI/workflow changes
- Remove obsolete test cases
- Consolidate redundant test cases
- Update test data references

**Best Practices:**
- Version control test cases
- Link test cases to requirements (traceability)
- Use modular test case design
- Schedule periodic reviews (quarterly)
- Track test case effectiveness

**Maintenance Triggers:**
- Requirement changes
- UI redesigns
- Failed test case reviews
- New feature additions
- Defect patterns discovered`,
            realTimeExample: `E-Commerce Project Maintenance Cycle:

**Monthly Review (Sprint Planning):**
- Reviewed 150 regression test cases
- Found 12 obsolete (feature removed) â†’ Archived
- Found 8 outdated (UI changed) â†’ Updated

**Quarterly Deep Cleanup:**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q3 2024 TEST CASE MAINTENANCE REPORT    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Test Cases: 450                   â”‚
â”‚ Active: 420 (93%)                       â”‚
â”‚ Archived: 30 (7%)                       â”‚
â”‚                                         â”‚
â”‚ Updated this quarter: 45                â”‚
â”‚ New additions: 38                       â”‚
â”‚ Consolidated (merged): 15               â”‚
â”‚                                         â”‚
â”‚ Pass Rate Impact: 92% â†’ 95%             â”‚
â”‚ (Fewer false failures after updates)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
          },
          {
            question: "What's the difference between test case and test scenario?",
            answer: `**Test Scenario:**
- High-level description of what to test
- Business-oriented, end-to-end flow
- One line or brief description
- Derived from requirements/user stories
- Example: "Verify user can complete purchase"

**Test Case:**
- Detailed steps to execute
- Specific inputs and expected outputs
- Includes preconditions, test data
- Multiple test cases per scenario
- Example: "Purchase with valid credit card, qty=1"

**Relationship:**
Test Scenario â†’ Multiple Test Cases

**Hierarchy:**
Requirement â†’ Test Scenario â†’ Test Cases â†’ Test Steps`,
            realTimeExample: `Banking Login Feature:

**Test Scenario (1 line):**
"Verify user login functionality"

**Test Cases Derived (Multiple):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO: Verify user login             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TC-001: Login with valid credentials    â”‚
â”‚ TC-002: Login with invalid password     â”‚
â”‚ TC-003: Login with unregistered email   â”‚
â”‚ TC-004: Login with empty fields         â”‚
â”‚ TC-005: Login with locked account       â”‚
â”‚ TC-006: Login after password reset      â”‚
â”‚ TC-007: Login with "Remember Me"        â”‚
â”‚ TC-008: Login timeout handling          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each test case has:
- Preconditions
- Step-by-step instructions
- Specific test data
- Expected results`
          },
          {
            question: "How many test cases per requirement is ideal?",
            answer: `There's no fixed number - it depends on requirement complexity. Use risk-based approach:

**General Guidelines:**
- Simple requirement: 3-5 test cases
- Medium complexity: 5-15 test cases
- Complex requirement: 15-30+ test cases

**Factors Affecting Count:**
- Number of input fields
- Business rules involved
- Integration points
- User roles/permissions
- Error handling scenarios

**Coverage Types Needed:**
- Positive scenarios (happy path)
- Negative scenarios (error handling)
- Boundary conditions (BVA)
- Edge cases
- Security scenarios (if applicable)

**Quality over Quantity:**
- Focus on coverage, not count
- Avoid redundant test cases
- Each test case should add value`,
            realTimeExample: `Password Reset Feature Analysis:

**Requirement:** User should be able to reset password via email

**Complexity Analysis:**
- 1 input field (email)
- 5 validation rules
- 2 integration points (email service, database)
- 3 error scenarios

**Test Cases (12 total):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POSITIVE (4):                           â”‚
â”‚ â€¢ Valid email, link received            â”‚
â”‚ â€¢ Reset with new valid password         â”‚
â”‚ â€¢ Login with new password               â”‚
â”‚ â€¢ Reset request while logged out        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NEGATIVE (5):                           â”‚
â”‚ â€¢ Invalid email format                  â”‚
â”‚ â€¢ Unregistered email                    â”‚
â”‚ â€¢ Expired reset link                    â”‚
â”‚ â€¢ Already used reset link               â”‚
â”‚ â€¢ New password doesn't meet criteria    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ EDGE CASES (3):                         â”‚
â”‚ â€¢ Request multiple times rapidly        â”‚
â”‚ â€¢ Very long email address               â”‚
â”‚ â€¢ Reset during active session           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Coverage: 100% of scenarios with 12 test cases`
          }
        ],
        difficulty: "Beginner"
      },
      {
        id: "TC-002",
        question: "What are test design techniques? Explain BVA and EP.",
        shortAnswer: "Test design techniques help create effective test cases. BVA tests boundary values. EP divides data into equivalent partitions.",
        detailedAnswer: `**Boundary Value Analysis (BVA):**
Tests at the edges of input ranges where defects commonly occur.

For age field (18-60):
- Test: 17, 18, 19, 59, 60, 61

**Equivalence Partitioning (EP):**
Divides input into groups where system behaves similarly, test one from each.

For age field (18-60):
- Invalid: < 18 (test: 10)
- Valid: 18-60 (test: 35)
- Invalid: > 60 (test: 75)

Other techniques:
- Decision Table: For multiple conditions
- State Transition: For state-based systems
- Use Case Testing: Based on user scenarios`,
        stlcAgileExample: {
          context: "In Agile, apply these techniques when writing test cases from acceptance criteria.",
          realWorldScenario: "For Insurance Premium Calculator with age 25-65, use BVA and EP to ensure complete coverage.",
          sampleDocument: `TEST DESIGN - Insurance Premium Calculator
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REQUIREMENT: Calculate premium for age 25-65

EQUIVALENCE PARTITIONING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Invalid    â”‚  Valid       â”‚   Invalid      â”‚
â”‚ (< 25)     â”‚  (25-65)     â”‚   (> 65)       â”‚
â”‚ Test: 20   â”‚  Test: 40    â”‚   Test: 70     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BOUNDARY VALUE ANALYSIS:
      24    25    26    ...    64    65    66
       â–²     â–²     â–²            â–²     â–²     â–²
    Invalid                           Invalid
         Valid Boundaries
         
TEST CASES DERIVED:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TC-001   â”‚ Age = 24 â†’ Error "Min age 25" â”‚
â”‚ TC-002   â”‚ Age = 25 â†’ Premium calculated â”‚
â”‚ TC-003   â”‚ Age = 26 â†’ Premium calculated â”‚
â”‚ TC-004   â”‚ Age = 40 â†’ Premium calculated â”‚
â”‚ TC-005   â”‚ Age = 64 â†’ Premium calculated â”‚
â”‚ TC-006   â”‚ Age = 65 â†’ Premium calculated â”‚
â”‚ TC-007   â”‚ Age = 66 â†’ Error "Max age 65" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COVERAGE ANALYSIS:
EP Coverage: 3/3 partitions (100%)
BVA Coverage: 6/6 boundaries (100%)
Total Test Cases: 7 (efficient coverage)`
        },
        agileJiraExample: {
          context: "Document test design technique in test case description for traceability.",
          realWorldScenario: "Add custom field 'Design Technique' to track which technique generated the test case.",
          sampleDocument: `JIRA CUSTOM FIELDS FOR DESIGN TECHNIQUES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TEST CASE: Age Validation - Lower Boundary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Design Technique: BVA (Lower Boundary)  â”‚
â”‚ Related Requirement: Premium Calculator â”‚
â”‚                                         â”‚
â”‚ Test Data:                              â”‚
â”‚ Age: 25 (minimum valid boundary)        â”‚
â”‚                                         â”‚
â”‚ Expected: Premium = $150/month          â”‚
â”‚                                         â”‚
â”‚ Labels: BVA, Boundary, Positive         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JQL FOR TECHNIQUE COVERAGE:
"Design Technique" = BVA AND project = INS
â†’ Returns all BVA-based test cases`
        },
        tips: [
          "Always combine BVA and EP for better coverage",
          "Mention specific examples with numbers",
          "Know when to use each technique"
        ],
        commonMistakes: [
          "Only testing middle values, missing boundaries",
          "Testing all values in a partition (defeats EP purpose)",
          "Not considering negative/positive boundaries"
        ],
        followUpQuestions: [
          {
            question: "How many boundary values should you test?",
            answer: `The number of boundary values depends on the approach:

**Two-Value Boundary Testing (Minimum):**
- Test AT the boundary only
- Example: For range 1-100, test: 1, 100
- 2 values per boundary

**Three-Value Boundary Testing (Recommended):**
- Test AT, BELOW, and ABOVE boundary
- Example: For range 1-100, test: 0, 1, 2, 99, 100, 101
- 6 total values (3 per boundary)

**Robust Boundary Testing:**
- Include min-1, min, min+1, max-1, max, max+1
- Plus nominal value in middle
- 7 total values

**Best Practice:**
For critical fields, use three-value approach.
For less critical, two-value may suffice.`,
            realTimeExample: `Age Field Validation (18-65 allowed):

**Two-Value Approach (4 tests):**
- Lower boundary: 18 (valid)
- Upper boundary: 65 (valid)
- Below lower: 17 (invalid)
- Above upper: 66 (invalid)

**Three-Value Approach (6 tests - Recommended):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VALUE â”‚ EXPECTED    â”‚ TYPE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 17    â”‚ Error msg   â”‚ Below lower bound â”‚
â”‚ 18    â”‚ Accepted    â”‚ AT lower bound    â”‚
â”‚ 19    â”‚ Accepted    â”‚ Above lower bound â”‚
â”‚ 64    â”‚ Accepted    â”‚ Below upper bound â”‚
â”‚ 65    â”‚ Accepted    â”‚ AT upper bound    â”‚
â”‚ 66    â”‚ Error msg   â”‚ Above upper bound â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

We used 6 tests in our Insurance project
for the age field - caught 2 off-by-one bugs!`
          },
          {
            question: "What if there are multiple input fields?",
            answer: `For multiple input fields, use a combination approach:

**Option 1: All Combinations (Exhaustive)**
- Test every boundary of every field together
- Very thorough but time-consuming
- Use for critical, high-risk features

**Option 2: One Field at a Time (Practical)**
- Keep other fields at valid values
- Test boundaries of one field
- Repeat for each field
- Most common approach

**Option 3: Pairwise/Orthogonal (Optimal)**
- Cover all pairs of boundary conditions
- Uses mathematical techniques
- Tools: PICT, AllPairs

**Risk-Based Selection:**
- Identify fields that interact
- Focus on field combinations that matter
- Document coverage decisions`,
            realTimeExample: `Loan Application Form (3 fields):
- Age: 25-65
- Amount: $1,000-$500,000  
- Term: 1-30 years

**One at a Time Approach (18 tests):**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEST â”‚ AGE   â”‚ AMOUNT    â”‚ TERM â”‚ TYPE  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1    â”‚ 24    â”‚ $10,000   â”‚ 5    â”‚ Age-  â”‚
â”‚ 2    â”‚ 25    â”‚ $10,000   â”‚ 5    â”‚ Age=  â”‚
â”‚ 3    â”‚ 26    â”‚ $10,000   â”‚ 5    â”‚ Age+  â”‚
â”‚ 4    â”‚ 40    â”‚ $999      â”‚ 5    â”‚ Amt-  â”‚
â”‚ 5    â”‚ 40    â”‚ $1,000    â”‚ 5    â”‚ Amt=  â”‚
â”‚ 6    â”‚ 40    â”‚ $1,001    â”‚ 5    â”‚ Amt+  â”‚
â”‚ ...  â”‚       â”‚           â”‚      â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**High-Risk Combinations Added:**
- Young age + Maximum amount + Long term
- Elderly age + Minimum amount + Short term
(These caught 1 calculation bug in our project)`
          },
          {
            question: "How do you apply BVA to non-numeric fields?",
            answer: `BVA applies to any field with boundaries, not just numbers:

**String Length Boundaries:**
- Password: min 8, max 20 characters
- Test: 7 chars, 8 chars, 9 chars, 19 chars, 20 chars, 21 chars

**Date Boundaries:**
- Valid date range: Jan 1, 2020 - Dec 31, 2024
- Test: Dec 31, 2019 | Jan 1, 2020 | Dec 31, 2024 | Jan 1, 2025

**Dropdown/Selection:**
- First option, last option
- Empty selection if allowed

**File Upload:**
- Min file size, max file size
- Allowed extensions at boundaries

**Time-Based:**
- Session timeout boundaries
- Booking time slots`,
            realTimeExample: `Username Field Validation:
Requirement: 3-20 characters, alphanumeric only

**String Length BVA:**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LENGTH â”‚ VALUE              â”‚ EXPECTED  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2      â”‚ "ab"              â”‚ Error     â”‚
â”‚ 3      â”‚ "abc"             â”‚ Valid     â”‚
â”‚ 4      â”‚ "abcd"            â”‚ Valid     â”‚
â”‚ 19     â”‚ "abcdefghij..."   â”‚ Valid     â”‚
â”‚ 20     â”‚ "abcdefghijk..."  â”‚ Valid     â”‚
â”‚ 21     â”‚ "abcdefghijkl..." â”‚ Error     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Date Field (Booking System):**
Available dates: Today to 90 days ahead

| DATE            | EXPECTED      |
|-----------------|---------------|
| Yesterday       | Error (past)  |
| Today           | Valid         |
| Tomorrow        | Valid         |
| Day 89          | Valid         |
| Day 90          | Valid         |
| Day 91          | Error (too far)|

Caught bug: System allowed day 91 bookings!`
          }
        ],
        difficulty: "Intermediate"
      }
    ]
  },
  {
    category: "Agile & Scrum Testing",
    icon: "ğŸ”",
    description: "Questions about testing in Agile environment",
    questions: [
      {
        id: "AGILE-001",
        question: "What are the key Scrum ceremonies and tester's role in each?",
        shortAnswer: "Sprint Planning, Daily Standup, Sprint Review, Retrospective. Testers participate in all, providing testing perspective.",
        detailedAnswer: `**1. Sprint Planning:**
- Tester Role: Review stories for testability, estimate testing effort, identify test dependencies
- Output: Test tasks added to sprint backlog

**2. Daily Stand-up:**
- Tester Role: Share testing progress, blockers, defects found
- Focus: Yesterday's testing, today's plan, any impediments

**3. Sprint Review:**
- Tester Role: Demo tested features, share quality metrics, highlight critical bugs
- Output: Stakeholder feedback on quality

**4. Sprint Retrospective:**
- Tester Role: Suggest process improvements, discuss defect patterns
- Output: Action items for better testing`,
        stlcAgileExample: {
          context: "Testers are integral part of Scrum team, not separate QA team.",
          realWorldScenario: "In a Banking app sprint, tester attends planning to flag that payment testing needs mock server setup.",
          sampleDocument: `TESTER PARTICIPATION - SPRINT CEREMONIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SPRINT PLANNING (2-4 hours):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TESTER CONTRIBUTIONS:                   â”‚
â”‚ âœ“ Review acceptance criteria clarity    â”‚
â”‚ âœ“ Flag missing testability requirements â”‚
â”‚ âœ“ Estimate testing effort per story     â”‚
â”‚ âœ“ Identify test data/env needs          â”‚
â”‚ âœ“ Break down test tasks                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example: Story "Fund Transfer"
Developer estimate: 5 points
Tester estimate: +2 points for integration testing
Final: 7 points with test tasks

DAILY STANDUP (15 min):
"Yesterday: Tested US-301, found 2 bugs
 Today: Will retest after fix, start US-302
 Blocker: Test server down since morning"

SPRINT REVIEW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QA Metrics Shared:                      â”‚
â”‚ â€¢ Test cases executed: 45/50 (90%)      â”‚
â”‚ â€¢ Pass rate: 42/45 (93%)                â”‚
â”‚ â€¢ Open bugs: 3 (1 Critical, 2 Medium)   â”‚
â”‚ â€¢ Automation added: 15 new tests        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RETROSPECTIVE:
What went well: Early testing caught design issue
Improve: Need better test data management
Action: Create test data refresh script`
        },
        agileJiraExample: {
          context: "Use Jira boards and dashboards to track testing activities in ceremonies.",
          realWorldScenario: "During standup, QA opens Jira board filtered by 'In Testing' to discuss status.",
          sampleDocument: `JIRA FOR CEREMONY SUPPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SPRINT PLANNING - Story Selection View:
Filter: Sprint = "Sprint 15" AND status = "To Do"
Fields shown: Summary, Points, AC Count, Test Tasks

DAILY STANDUP - QA Quick Filters:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My In Progress â”‚ Blocked â”‚ Done Today   â”‚
â”‚     â–ˆâ–ˆâ–ˆâ–ˆ       â”‚   â–ˆâ–ˆ    â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Filter: assignee = currentUser() 
        AND Sprint = activeSprint()
        AND status changed during -1d

SPRINT REVIEW - Dashboard Gadget:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Sprint 15 Quality Report         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stories Tested: 8/8                     â”‚
â”‚ Test Cases: 45 Passed, 3 Failed, 2 NA   â”‚
â”‚ Bugs Found: 12 | Fixed: 10 | Open: 2    â”‚
â”‚ Automation: +15 new | 120 total         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Emphasize tester as team member, not gatekeeper",
          "Mention shift-left testing approach",
          "Be ready with examples of your ceremony participation"
        ],
        commonMistakes: [
          "Saying testing happens after development in Agile",
          "Not mentioning retrospective improvements",
          "Describing testers as separate from the team"
        ],
        followUpQuestions: [
          "How do you handle incomplete stories at sprint end?",
          "What if developers finish late, leaving no time for testing?",
          "How do you estimate testing effort in planning?"
        ],
        difficulty: "Beginner"
      },
      {
        id: "AGILE-002",
        question: "Explain the Jira issue hierarchy and when to use each level.",
        shortAnswer: "Epic > Story > Task > Sub-task > Bug. Epics for features, Stories for user requirements, Tasks for technical work.",
        detailedAnswer: `**Jira Issue Hierarchy:**

1. **Epic**: Large feature spanning multiple sprints
   - Example: "User Management System"
   
2. **Story**: User requirement delivering value
   - Example: "As a user, I want to reset password"
   
3. **Task**: Technical work within a story
   - Example: "Create password reset API"
   
4. **Sub-task**: Smaller work items
   - Example: "Write unit tests for reset API"
   
5. **Bug**: Defects found during testing
   - Example: "Reset link expires immediately"

Use the right level based on:
- Scope and complexity
- Who needs visibility
- How work is tracked/reported`,
        stlcAgileExample: {
          context: "Test activities map to different Jira levels based on scope.",
          realWorldScenario: "For Telecom billing feature: Epic for Billing System, Stories for each billing function, Sub-tasks for test cases.",
          sampleDocument: `JIRA HIERARCHY - TELECOM BILLING PROJECT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EPIC: TELE-EP-001 Billing & Invoicing System
â”œâ”€â”€ STORY: TELE-101 Generate Monthly Invoice
â”‚   â”œâ”€â”€ TASK: TELE-101-DEV: API development
â”‚   â”œâ”€â”€ TASK: TELE-101-TEST: Test planning
â”‚   â”‚   â”œâ”€â”€ SUB-TASK: Write test cases
â”‚   â”‚   â”œâ”€â”€ SUB-TASK: Prepare test data
â”‚   â”‚   â””â”€â”€ SUB-TASK: Execute tests
â”‚   â””â”€â”€ BUG: TELE-BUG-201: Tax calculation wrong
â”‚
â”œâ”€â”€ STORY: TELE-102 View Invoice History
â”‚   â”œâ”€â”€ TASK: TELE-102-UI: Frontend development
â”‚   â”œâ”€â”€ TASK: TELE-102-TEST: Test execution
â”‚   â””â”€â”€ BUG: TELE-BUG-202: Pagination missing
â”‚
â””â”€â”€ STORY: TELE-103 Download Invoice PDF

WHEN TO USE WHAT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Level        â”‚ Test Use Case               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Epic         â”‚ Overall test strategy       â”‚
â”‚ Story        â”‚ Feature test plan           â”‚
â”‚ Task         â”‚ Test execution              â”‚
â”‚ Sub-task     â”‚ Individual test cases       â”‚
â”‚ Bug          â”‚ Defects found               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "Proper hierarchy enables reporting and traceability.",
          realWorldScenario: "Manager views Epic to see overall feature status. QA views Story to see test progress. Developer views Bug to see defects assigned.",
          sampleDocument: `JIRA REPORTING BY HIERARCHY LEVEL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EPIC LEVEL REPORT (For Management):
JQL: type = Epic AND project = TELE

Shows: Feature completion, Story count, Timeline

STORY LEVEL REPORT (For Product Owner):
JQL: type = Story AND "Epic Link" = TELE-EP-001

Shows: Story status, Points, Linked bugs

BUG LEVEL REPORT (For QA):
JQL: type = Bug AND Sprint = activeSprint()
     ORDER BY priority DESC

Shows: All sprint bugs by priority

DASHBOARD CONFIGURATION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPIC PROGRESS          â”‚ STORIES BY STATUSâ”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 85%        â”‚ Done: 5          â”‚
â”‚                        â”‚ In Progress: 2   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BUGS BY SEVERITY       â”‚ TEST COVERAGE    â”‚
â”‚ ğŸ”´ Critical: 1         â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 92%   â”‚
â”‚ ğŸŸ  High: 3             â”‚ 46/50 test cases â”‚
â”‚ ğŸŸ¡ Medium: 5           â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Explain with specific examples",
          "Mention how hierarchy enables reporting",
          "Know your organization's customizations"
        ],
        commonMistakes: [
          "Confusing Task with Story",
          "Creating Epics for small features",
          "Not linking bugs to stories"
        ],
        followUpQuestions: [
          "Can a Task exist without a Story?",
          "How do you handle cross-story bugs?",
          "When would you use a Spike instead of a Story?"
        ],
        difficulty: "Beginner"
      },
      {
        id: "AGILE-003",
        question: "How do you decide which test cases to automate?",
        shortAnswer: "Automate stable, repetitive, high-risk tests. Consider ROI - automation cost vs manual effort saved over time.",
        detailedAnswer: `**Criteria for Automation:**

âœ… **Good Candidates:**
- Smoke tests (run every build)
- Regression tests (run frequently)
- Data-driven tests (many combinations)
- Critical business flows
- Stable features (low change frequency)

âŒ **Avoid Automating:**
- One-time tests
- Exploratory testing
- Frequently changing UI
- Tests requiring human judgment
- Low ROI tests

**ROI Calculation:**
Automation ROI = (Manual Cost Ã— Executions) - Automation Cost
If positive, automate!

**Coverage Target:**
Typically 60-80% of regression suite automated`,
        stlcAgileExample: {
          context: "In Agile STLC, automation decisions are made during test planning and sprint automation phases.",
          realWorldScenario: "For E-commerce checkout, automate happy path and payment flows. Keep exploratory tests manual.",
          sampleDocument: `AUTOMATION DECISION MATRIX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

E-COMMERCE CHECKOUT - AUTOMATION ANALYSIS

USER STORY: EC-US-020 Checkout Process

TEST CASE ANALYSIS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Case          â”‚ Runs/Sprt â”‚ Stable?  â”‚ Complex â”‚ ROI   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TC-001: Add to cartâ”‚    20     â”‚   Yes    â”‚   Low   â”‚ High  â”‚ âœ…
â”‚ TC-002: Valid pay  â”‚    20     â”‚   Yes    â”‚   Med   â”‚ High  â”‚ âœ…
â”‚ TC-003: Card error â”‚    10     â”‚   Yes    â”‚   Low   â”‚ High  â”‚ âœ…
â”‚ TC-004: UI layout  â”‚     5     â”‚   No     â”‚   High  â”‚ Low   â”‚ âŒ
â”‚ TC-005: Exploratoryâ”‚    N/A    â”‚   N/A    â”‚   N/A   â”‚ N/A   â”‚ âŒ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

ROI CALCULATION:
TC-001: Add to Cart
â”œâ”€â”€ Manual: 10 min Ã— 20 runs Ã— 12 sprints = 2400 min/year
â”œâ”€â”€ Automation: 2 hours initial + 30 min/sprint maint
â”œâ”€â”€ Automation total: 120 + (30 Ã— 12) = 480 min/year
â””â”€â”€ SAVINGS: 1920 min/year = 32 hours! âœ…

AUTOMATION RECOMMENDATION:
Automate: 3 of 5 test cases (60%)
Keep Manual: UI layout, exploratory testing`
        },
        agileJiraExample: {
          context: "Track automation status in Jira with custom fields and workflows.",
          realWorldScenario: "Add 'Automation Status' field: Manual, To Automate, Automated, Not Suitable.",
          sampleDocument: `JIRA AUTOMATION TRACKING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CUSTOM FIELD: Automation Status
Options:
- Manual (default)
- To Automate
- In Progress
- Automated
- Not Suitable

AUTOMATION WORKFLOW:
Manual â†’ To Automate â†’ In Progress â†’ Automated
                    â†˜ Not Suitable

DASHBOARD - AUTOMATION COVERAGE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SPRINT 15 AUTOMATION STATUS        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Automated    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 65%       â”‚
â”‚ To Automate  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 15%       â”‚
â”‚ Manual       â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 15%       â”‚
â”‚ Not Suitable â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  5%       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JQL FOR AUTOMATION BACKLOG:
"Automation Status" = "To Automate" 
AND Sprint = activeSprint()
ORDER BY priority DESC`
        },
        tips: [
          "Always mention ROI - shows business thinking",
          "Give specific automation percentages (60-80%)",
          "Know the tools and frameworks used"
        ],
        commonMistakes: [
          "Saying 'automate everything'",
          "Not considering maintenance cost",
          "Ignoring test stability before automating"
        ],
        followUpQuestions: [
          "What's your automation to manual ratio?",
          "How do you handle flaky automated tests?",
          "When would you de-automate a test?"
        ],
        difficulty: "Intermediate"
      }
    ]
  },
  {
    category: "Defect Management",
    icon: "ğŸ›",
    description: "Questions about finding, reporting, and tracking defects",
    questions: [
      {
        id: "DEF-001",
        question: "How do you write a good bug report?",
        shortAnswer: "Include title, steps to reproduce, expected vs actual result, environment, severity, priority, and attachments.",
        detailedAnswer: `**Essential Bug Report Components:**

1. **Title**: Clear, specific summary
2. **Environment**: Browser, OS, version, build
3. **Preconditions**: Setup before reproducing
4. **Steps to Reproduce**: Numbered, specific steps
5. **Actual Result**: What happened
6. **Expected Result**: What should happen
7. **Severity**: Critical/High/Medium/Low
8. **Priority**: P1/P2/P3/P4
9. **Attachments**: Screenshots, videos, logs

**Good Title Example:**
âŒ "Login not working"
âœ… "Login fails with 500 error when email contains '+' symbol"`,
        stlcAgileExample: {
          context: "In Agile, bugs are linked to stories and fixed within the same sprint if possible.",
          realWorldScenario: "Banking app fund transfer shows wrong balance - critical bug blocking story completion.",
          sampleDocument: `BUG REPORT - BANKING APPLICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BUG ID: BANK-BUG-1045
TITLE: Fund Transfer Shows Incorrect Balance After Transaction

LINKED STORY: BANK-301 Fund Transfer
REPORTER: QA Analyst | DATE: 2024-01-15
SEVERITY: Critical | PRIORITY: P1

ENVIRONMENT:
â”œâ”€â”€ Application: Banking Portal v2.3.1
â”œâ”€â”€ Browser: Chrome 120.0.6099.109
â”œâ”€â”€ OS: Windows 11
â”œâ”€â”€ Test Server: qa-bank.company.com
â””â”€â”€ Test Data: Account A123, B456

PRECONDITIONS:
1. User logged in with valid credentials
2. Account A123 balance: $10,000
3. Account B456 balance: $5,000

STEPS TO REPRODUCE:
1. Navigate to Fund Transfer page
2. Select Source Account: A123
3. Select Destination Account: B456
4. Enter Amount: $500
5. Click "Transfer" button
6. Observe confirmation page

ACTUAL RESULT:
âœ— Source Account shows: $10,000 (unchanged)
âœ— Destination Account shows: $5,000 (unchanged)
âœ— Transaction appears in history
âœ— Database shows correct balances

EXPECTED RESULT:
âœ“ Source Account should show: $9,500
âœ“ Destination Account should show: $5,500
âœ“ UI should refresh with correct balances

ROOT CAUSE ANALYSIS:
UI not refreshing after API success response

ATTACHMENTS:
ğŸ“ Screenshot_balance_error.png
ğŸ“ Network_log.har
ğŸ“ Console_errors.txt`
        },
        agileJiraExample: {
          context: "Configure Jira with mandatory fields for consistent bug reporting.",
          realWorldScenario: "Jira workflow requires Environment, Steps to Reproduce before bug can be submitted.",
          sampleDocument: `JIRA BUG TEMPLATE CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROJECT SETTINGS > ISSUE TYPES > BUG

REQUIRED FIELDS:
â”œâ”€â”€ Summary (Title)
â”œâ”€â”€ Environment (Custom Field - dropdown)
â”‚   Options: Chrome, Firefox, Safari, Mobile
â”œâ”€â”€ Steps to Reproduce (Text Area)
â”œâ”€â”€ Expected Result (Text Area)
â”œâ”€â”€ Actual Result (Text Area)
â”œâ”€â”€ Severity (Custom Field)
â”‚   Options: Critical, High, Medium, Low
â””â”€â”€ Attachments (Minimum 1 required)

OPTIONAL BUT RECOMMENDED:
â”œâ”€â”€ Affected Version
â”œâ”€â”€ Reproducibility (Always, Sometimes, Rarely)
â”œâ”€â”€ Workaround Available (Yes/No)
â””â”€â”€ Root Cause (Text - filled by dev)

AUTOMATION RULE:
IF Severity = Critical 
THEN @mention QA Lead and Dev Lead
AND set Priority = P1

SAMPLE JIRA BUG SCREEN:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BANK-BUG-1045                           â”‚
â”‚ Fund Transfer Shows Incorrect Balance   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type: Bug  Severity: Critical  P1       â”‚
â”‚ Status: Open  Assignee: @developer      â”‚
â”‚ Sprint: Sprint 15  Story: BANK-301      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Environment: Chrome 120 / Windows 11    â”‚
â”‚ Affected Version: 2.3.1                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Steps to Reproduce:                     â”‚
â”‚ 1. Navigate to Fund Transfer...         â”‚
â”‚ [Expand to see all]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Always include screenshots or videos",
          "Be specific - no assumptions",
          "Distinguish severity from priority"
        ],
        commonMistakes: [
          "Vague titles like 'system crashes'",
          "Missing steps to reproduce",
          "Not providing environment details",
          "Confusing severity and priority"
        ],
        followUpQuestions: [
          "What's the difference between severity and priority?",
          "How do you handle non-reproducible bugs?",
          "What if the bug is already known?"
        ],
        difficulty: "Beginner"
      }
    ]
  },
  {
    category: "Testing Types",
    icon: "ğŸ§ª",
    description: "Questions about different types of testing",
    questions: [
      {
        id: "TYPE-001",
        question: "What is the difference between Smoke and Regression testing?",
        shortAnswer: "Smoke is quick build verification. Regression ensures new changes don't break existing functionality.",
        detailedAnswer: `**Smoke Testing:**
- Purpose: Verify build is stable enough for testing
- Scope: Critical paths only (10-20 tests)
- When: After every build deployment
- Time: 15-30 minutes
- Also called: Build Verification Testing (BVT)

**Regression Testing:**
- Purpose: Ensure new changes don't break existing features
- Scope: Comprehensive test suite (100s of tests)
- When: Before release, after major changes
- Time: Hours to days
- Often automated for efficiency

**Key Difference:**
Smoke = "Does the build work at all?"
Regression = "Does everything still work?"`,
        stlcAgileExample: {
          context: "In Agile, smoke tests run on every deployment, regression before sprint end.",
          realWorldScenario: "E-commerce site runs 15 smoke tests after each build, full 200-test regression before release.",
          sampleDocument: `SMOKE vs REGRESSION - E-COMMERCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SMOKE TEST SUITE (15 tests, 20 min):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID      â”‚ Test Case            â”‚ Time   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SM-001  â”‚ Home page loads      â”‚ 30s    â”‚
â”‚ SM-002  â”‚ User can login       â”‚ 1min   â”‚
â”‚ SM-003  â”‚ Search works         â”‚ 45s    â”‚
â”‚ SM-004  â”‚ Add to cart works    â”‚ 1min   â”‚
â”‚ SM-005  â”‚ Checkout accessible  â”‚ 45s    â”‚
â”‚ SM-006  â”‚ Payment page loads   â”‚ 1min   â”‚
â”‚ SM-007  â”‚ Order confirmation   â”‚ 1min   â”‚
â”‚ ...     â”‚ (8 more critical)    â”‚ ...    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Run: Every build | Automated: 100%

REGRESSION TEST SUITE (200 tests, 8 hours):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Module          â”‚ Tests â”‚ Auto â”‚ Manual â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Management â”‚   35  â”‚  28  â”‚    7   â”‚
â”‚ Product Catalog â”‚   45  â”‚  40  â”‚    5   â”‚
â”‚ Shopping Cart   â”‚   30  â”‚  25  â”‚    5   â”‚
â”‚ Checkout/Pay    â”‚   40  â”‚  30  â”‚   10   â”‚
â”‚ Order Tracking  â”‚   25  â”‚  20  â”‚    5   â”‚
â”‚ Admin Features  â”‚   25  â”‚  15  â”‚   10   â”‚
â”‚ TOTAL           â”‚  200  â”‚ 158  â”‚   42   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Run: Pre-release | Automated: 79%

EXECUTION STRATEGY:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPRINT TIMELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Day 1-7: Development (Smoke after builds)
â”‚ Day 8-10: Testing (Manual + Automation) â”‚
â”‚ Day 11-12: Regression Suite             â”‚
â”‚ Day 13: Bug fixes + Retesting           â”‚
â”‚ Day 14: Release                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "Track smoke and regression as test cycles or test plans in Jira.",
          realWorldScenario: "Create separate test cycles for Smoke (daily) and Regression (sprint-end).",
          sampleDocument: `JIRA TEST MANAGEMENT - ZEPHYR SETUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TEST CYCLES:
â”œâ”€â”€ Smoke Test - Build 2.3.1.45
â”‚   â”œâ”€â”€ Status: Passed
â”‚   â”œâ”€â”€ Tests: 15/15 Passed
â”‚   â”œâ”€â”€ Duration: 18 min
â”‚   â””â”€â”€ Executed: 2024-01-15 14:30
â”‚
â”œâ”€â”€ Smoke Test - Build 2.3.1.46
â”‚   â”œâ”€â”€ Status: Failed
â”‚   â”œâ”€â”€ Tests: 13/15 Passed, 2 Failed
â”‚   â”œâ”€â”€ Duration: 22 min
â”‚   â””â”€â”€ Blocker: Payment page broken
â”‚
â””â”€â”€ Regression - Sprint 15 Release
    â”œâ”€â”€ Status: In Progress
    â”œâ”€â”€ Tests: 180/200 Executed
    â”œâ”€â”€ Passed: 170, Failed: 8, Blocked: 2
    â””â”€â”€ ETA: 4 hours remaining

DASHBOARD VIEW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     SMOKE TEST TREND (LAST 7 DAYS)      â”‚
â”‚  âœ“   âœ“   âœ“   âœ“   âœ—   âœ“   âœ“             â”‚
â”‚ Mon Tue Wed Thu Fri Sat Sun             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     REGRESSION PROGRESS                 â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 90%             â”‚
â”‚  180/200 executed                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Know the scope difference (critical vs comprehensive)",
          "Mention automation - both should be automated",
          "Be ready with numbers (10-20 smoke, 100+ regression)"
        ],
        commonMistakes: [
          "Saying smoke tests everything",
          "Not automating regression tests",
          "Running full regression on every build"
        ],
        followUpQuestions: [
          "How many smoke tests is ideal?",
          "Can smoke test replace regression?",
          "How do you prioritize regression tests?"
        ],
        difficulty: "Beginner"
      },
      {
        id: "TYPE-002",
        question: "What is Integration Testing and when do you perform it?",
        shortAnswer: "Testing interaction between integrated modules/components. Done after unit testing, before system testing.",
        detailedAnswer: `**Integration Testing:**
Tests how different modules work together when integrated.

**Types:**
1. **Big Bang**: Integrate all at once, test together
2. **Incremental**: Integrate and test step by step
   - Top-Down: Start from top module
   - Bottom-Up: Start from bottom module
   - Sandwich/Hybrid: Both directions

**When to Perform:**
- After unit testing of individual modules
- When modules are ready to integrate
- Before system testing

**Focus Areas:**
- Data flow between modules
- API contracts and responses
- Database interactions
- Third-party service integrations`,
        stlcAgileExample: {
          context: "In Agile, integration testing happens as features are developed and integrated.",
          realWorldScenario: "E-commerce: Test integration between Cart module and Payment Gateway API.",
          sampleDocument: `INTEGRATION TEST PLAN - CHECKOUT FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODULES INVOLVED:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    UI (React)                           â”‚
â”‚         â”‚                               â”‚
â”‚         â–¼                               â”‚
â”‚    Cart Service â”€â”€â–º Product Service     â”‚
â”‚         â”‚                               â”‚
â”‚         â–¼                               â”‚
â”‚    Payment Gateway (Stripe API)         â”‚
â”‚         â”‚                               â”‚
â”‚         â–¼                               â”‚
â”‚    Order Service â”€â”€â–º Notification       â”‚
â”‚         â”‚                               â”‚
â”‚         â–¼                               â”‚
â”‚    Database (PostgreSQL)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INTEGRATION TEST CASES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INT-001: Cart to Payment Integration       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Verify cart data passes correctly to       â”‚
â”‚ payment gateway with amount, currency      â”‚
â”‚                                            â”‚
â”‚ Modules: Cart Service â†’ Stripe API         â”‚
â”‚                                            â”‚
â”‚ Test Data:                                 â”‚
â”‚ - Cart total: $150.00                      â”‚
â”‚ - Currency: USD                            â”‚
â”‚                                            â”‚
â”‚ Expected:                                  â”‚
â”‚ - Stripe receives exact amount             â”‚
â”‚ - Payment intent created                   â”‚
â”‚ - Client secret returned                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INT-002: Payment to Order Integration      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Verify successful payment creates order    â”‚
â”‚ with correct details                       â”‚
â”‚                                            â”‚
â”‚ Modules: Stripe Webhook â†’ Order Service    â”‚
â”‚                                            â”‚
â”‚ Test Data:                                 â”‚
â”‚ - Payment status: succeeded                â”‚
â”‚ - Transaction ID: pi_123abc               â”‚
â”‚                                            â”‚
â”‚ Expected:                                  â”‚
â”‚ - Order created with status "Confirmed"    â”‚
â”‚ - Transaction ID stored                    â”‚
â”‚ - Confirmation email triggered             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "Track integration tests as separate test type in Jira.",
          realWorldScenario: "Label integration test cases with 'Integration' and link to multiple stories.",
          sampleDocument: `JIRA INTEGRATION TEST STRUCTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ISSUE TYPE: Test Case
LABELS: Integration, API, E2E

SAMPLE TEST CASE IN JIRA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INT-001: Cart to Payment Integration    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type: Test Case                         â”‚
â”‚ Labels: Integration, API                â”‚
â”‚                                         â”‚
â”‚ Linked Stories:                         â”‚
â”‚ - SHOP-110: Add to Cart                 â”‚
â”‚ - SHOP-120: Payment Processing          â”‚
â”‚                                         â”‚
â”‚ Component: Checkout                     â”‚
â”‚ Test Level: Integration                 â”‚
â”‚ Automation: Automated (Postman)         â”‚
â”‚                                         â”‚
â”‚ Execution History:                      â”‚
â”‚ Sprint 15: âœ“ Passed                     â”‚
â”‚ Sprint 14: âœ— Failed (timeout issue)     â”‚
â”‚ Sprint 13: âœ“ Passed                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JQL FOR INTEGRATION TESTS:
labels = Integration AND type = "Test Case"
AND project = SHOP`
        },
        tips: [
          "Know the different integration approaches",
          "Mention API testing as key integration testing",
          "Be ready to explain stubs and drivers"
        ],
        commonMistakes: [
          "Confusing with system testing",
          "Not testing error scenarios in integration",
          "Missing API contract testing"
        ],
        followUpQuestions: [
          "What's the difference between integration and system testing?",
          "How do you test when dependent service is unavailable?",
          "What tools do you use for API integration testing?"
        ],
        difficulty: "Intermediate"
      }
    ]
  },
  {
    category: "CI/CD & Automation",
    icon: "âš™ï¸",
    description: "Questions about continuous integration, delivery, and test automation",
    questions: [
      {
        id: "CICD-001",
        question: "How do you integrate automated tests into a CI/CD pipeline?",
        shortAnswer: "Configure CI tool to trigger tests on code changes. Define test stages, quality gates, and notifications.",
        detailedAnswer: `**CI/CD Test Integration Steps:**

1. **Select CI Tool**: Jenkins, GitLab CI, GitHub Actions
2. **Configure Pipeline Stages**:
   - Build â†’ Unit Tests â†’ Integration â†’ Deploy â†’ E2E
3. **Set Up Test Execution**:
   - Define test commands
   - Configure test reports
4. **Define Quality Gates**:
   - Fail build on test failure
   - Code coverage thresholds
5. **Notifications**:
   - Slack/Email on failure

**Best Practices:**
- Fast tests early (unit < 5 min)
- Parallel test execution
- Flaky test quarantine
- Test environment management`,
        stlcAgileExample: {
          context: "In Agile STLC, CI/CD integration is part of Sprint Automation phase.",
          realWorldScenario: "Insurance portal pipeline runs smoke tests on every PR, full regression nightly.",
          sampleDocument: `CI/CD PIPELINE - INSURANCE PORTAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PIPELINE STAGES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODE PUSH                               â”‚
â”‚      â”‚                                   â”‚
â”‚      â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚   BUILD   â”‚ â†’ Compile, Package        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚UNIT TESTS â”‚ â†’ 100+ tests (2 min)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚SMOKE TESTSâ”‚ â†’ 15 critical (5 min)     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ DEPLOY QA â”‚ â†’ Auto-deploy to QA       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ API TESTS â”‚ â†’ 50 tests (10 min)       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NIGHTLY REGRESSION (Scheduled 2 AM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Full Regression: 200 tests (2 hours)     â”‚
â”‚ Security Scan: OWASP ZAP                 â”‚
â”‚ Performance: JMeter baseline             â”‚
â”‚ Report: Email + Slack                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUALITY GATES:
Gate 1: Unit tests 100% pass â†’ Proceed
Gate 2: Smoke tests pass â†’ Deploy to QA
Gate 3: Regression 95%+ â†’ Proceed to staging
Gate 4: No Critical bugs â†’ Production release

JENKINSFILE EXAMPLE:
\`\`\`groovy
pipeline {
  stages {
    stage('Build') {
      steps { sh 'mvn clean package' }
    }
    stage('Unit Tests') {
      steps { sh 'mvn test' }
      post {
        always { junit 'target/surefire-reports/*.xml' }
      }
    }
    stage('Smoke Tests') {
      steps { sh 'mvn verify -Dsuite=smoke' }
    }
    stage('Deploy QA') {
      when { branch 'develop' }
      steps { sh './deploy-qa.sh' }
    }
  }
}
\`\`\``
        },
        agileJiraExample: {
          context: "Link Jira to CI/CD for automatic status updates and build tracking.",
          realWorldScenario: "GitHub Actions updates Jira ticket status when tests pass.",
          sampleDocument: `JIRA CI/CD INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GITHUB ACTIONS â†’ JIRA SYNC:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Action: On PR Merge to develop          â”‚
â”‚                                         â”‚
â”‚ 1. Run tests                            â”‚
â”‚ 2. If pass â†’ Update Jira to "Done"      â”‚
â”‚ 3. If fail â†’ Comment on Jira ticket     â”‚
â”‚ 4. Add build link to Jira               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JIRA SMART COMMITS:
Commit: "SHOP-110 fix cart total calculation"
Result: Jira SHOP-110 gets linked commit

JIRA DASHBOARD - BUILD STATUS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       RECENT BUILDS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ #245 develop  âœ“ Passed   2 min ago      â”‚
â”‚ #244 feature  âœ— Failed   15 min ago     â”‚
â”‚ #243 develop  âœ“ Passed   1 hour ago     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linked Stories: 5 updated automatically â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Know specific CI tools and their syntax",
          "Mention quality gates and thresholds",
          "Explain parallel execution for speed"
        ],
        commonMistakes: [
          "Not failing build on test failure",
          "Running slow tests on every commit",
          "No notifications for failures"
        ],
        followUpQuestions: [
          "How do you handle flaky tests in CI?",
          "What's the ideal pipeline duration?",
          "How do you manage test environments?"
        ],
        difficulty: "Advanced"
      }
    ]
  },
  {
    category: "AI in Testing",
    icon: "ğŸ¤–",
    description: "Questions about AI-powered testing tools and techniques",
    questions: [
      {
        id: "AI-001",
        question: "How is AI used in software testing?",
        shortAnswer: "AI helps with test generation, self-healing locators, visual testing, bug prediction, and test optimization.",
        detailedAnswer: `**AI Applications in Testing:**

1. **Test Case Generation**:
   - Generate tests from requirements
   - Create edge case scenarios
   - Tools: ChatGPT, TestSigma AI

2. **Self-Healing Tests**:
   - Auto-update locators when UI changes
   - Reduce maintenance
   - Tools: Testim.io, Mabl

3. **Visual Testing**:
   - Detect UI anomalies
   - Compare screenshots
   - Tools: Applitools, Percy

4. **Bug Prediction**:
   - Identify bug-prone areas
   - Prioritize testing
   - Based on code changes

5. **Test Optimization**:
   - Select relevant tests
   - Reduce execution time
   - Smart test ordering`,
        stlcAgileExample: {
          context: "AI tools integrate into Agile STLC at test design and execution phases.",
          realWorldScenario: "Use ChatGPT to generate test cases from user stories, Testim for self-healing automation.",
          sampleDocument: `AI IN AGILE TESTING WORKFLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SPRINT ACTIVITIES WITH AI:

DAY 1-2: REQUIREMENT ANALYSIS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI TOOL: ChatGPT                        â”‚
â”‚ PURPOSE: Generate test scenarios        â”‚
â”‚                                         â”‚
â”‚ PROMPT:                                 â”‚
â”‚ "Generate test scenarios for user       â”‚
â”‚ story: As a customer, I want to apply   â”‚
â”‚ coupon codes at checkout so I get       â”‚
â”‚ discounts. Include positive, negative,  â”‚
â”‚ and edge cases."                        â”‚
â”‚                                         â”‚
â”‚ OUTPUT: 15 test scenarios generated     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DAY 3-6: TEST CASE DEVELOPMENT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI TOOL: TestSigma AI                   â”‚
â”‚ PURPOSE: Auto-generate test steps       â”‚
â”‚                                         â”‚
â”‚ INPUT: Natural language test            â”‚
â”‚ "Login and apply coupon SAVE20"         â”‚
â”‚                                         â”‚
â”‚ OUTPUT: Executable test script          â”‚
â”‚ with element identification             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DAY 7-12: TEST EXECUTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI TOOL: Testim.io                      â”‚
â”‚ PURPOSE: Self-healing automation        â”‚
â”‚                                         â”‚
â”‚ SCENARIO:                               â”‚
â”‚ Button class changed from               â”‚
â”‚ 'btn-apply' to 'coupon-apply-btn'       â”‚
â”‚                                         â”‚
â”‚ AI ACTION:                              â”‚
â”‚ âœ“ Detected locator failure              â”‚
â”‚ âœ“ Analyzed DOM for alternatives         â”‚
â”‚ âœ“ Auto-updated to new locator           â”‚
â”‚ âœ“ Test passed without manual fix        â”‚
â”‚                                         â”‚
â”‚ SAVINGS: 2 hours maintenance avoided    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        agileJiraExample: {
          context: "AI insights can be tracked and reported in Jira dashboards.",
          realWorldScenario: "Create custom fields to track AI-generated vs manual tests, AI-detected bugs.",
          sampleDocument: `JIRA AI TESTING METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CUSTOM FIELDS:
â”œâ”€â”€ Test Source: Manual | AI-Generated | Hybrid
â”œâ”€â”€ AI Tool Used: ChatGPT | Testim | Mabl
â”œâ”€â”€ Self-Healing Events: (count)
â””â”€â”€ AI Bug Detection: Yes/No

SPRINT AI INSIGHTS DASHBOARD:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     AI TESTING METRICS - SPRINT 15      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tests by Source:                        â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘ Manual: 60 (60%)         â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ AI-Gen: 30 (30%)         â”‚
â”‚ â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Hybrid: 10 (10%)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Self-Healing Events: 23 locator fixes   â”‚
â”‚ Time Saved: ~8 hours maintenance        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AI-Detected Bugs: 5 of 18 total (28%)   â”‚
â”‚ False Positives: 2                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JQL FOR AI TESTS:
"Test Source" = "AI-Generated" 
AND Sprint = activeSprint()`
        },
        tips: [
          "Know specific AI testing tools",
          "Mention practical benefits (time/cost saved)",
          "Be honest about limitations"
        ],
        commonMistakes: [
          "Claiming AI can replace testers",
          "Not mentioning false positives",
          "Overhyping AI capabilities"
        ],
        followUpQuestions: [
          "What are limitations of AI testing?",
          "How do you validate AI-generated tests?",
          "Can AI do exploratory testing?"
        ],
        difficulty: "Advanced"
      }
    ]
  },
  {
    category: "Metrics & Reporting",
    icon: "ğŸ“Š",
    description: "Questions about test metrics, reports, and quality assessment",
    questions: [
      {
        id: "MET-001",
        question: "What metrics do you track during testing?",
        shortAnswer: "Test execution status, pass/fail rate, defect metrics, test coverage, and velocity in Agile.",
        detailedAnswer: `**Key Testing Metrics:**

1. **Test Execution Metrics:**
   - Tests Executed vs Planned
   - Pass Rate = Passed / Total Executed
   - Blocked/Skipped count

2. **Defect Metrics:**
   - Defects Found vs Fixed
   - Severity Distribution
   - Defect Density = Bugs / KLOC
   - Defect Leakage to Prod

3. **Coverage Metrics:**
   - Requirement Coverage
   - Code Coverage (for automation)

4. **Agile Metrics:**
   - Velocity (story points completed)
   - Sprint Burndown
   - Escaped Defects

5. **Automation Metrics:**
   - Automation Coverage %
   - Automation ROI`,
        stlcAgileExample: {
          context: "In Agile STLC, metrics are reviewed daily and during sprint review.",
          realWorldScenario: "Banking app sprint tracks test pass rate, defects by severity, and automation coverage.",
          sampleDocument: `SPRINT TEST METRICS REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROJECT: Banking Portal | SPRINT: 15
PERIOD: Jan 1-14, 2024

TEST EXECUTION SUMMARY:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric          â”‚ Value  â”‚ Target  â”‚ âœ“/âœ—â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Planned   â”‚ 120    â”‚ 120     â”‚ âœ“  â”‚
â”‚ Executed        â”‚ 115    â”‚ 120     â”‚ âœ—  â”‚
â”‚ Passed          â”‚ 105    â”‚ -       â”‚ -  â”‚
â”‚ Failed          â”‚ 8      â”‚ < 10    â”‚ âœ“  â”‚
â”‚ Blocked         â”‚ 2      â”‚ 0       â”‚ âœ—  â”‚
â”‚ Pass Rate       â”‚ 91.3%  â”‚ > 95%   â”‚ âœ—  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DEFECT METRICS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Defects Found    â”‚ 18                   â”‚
â”‚ Defects Fixed    â”‚ 15                   â”‚
â”‚ Defects Open     â”‚ 3                    â”‚
â”‚ Reopen Rate      â”‚ 2/15 = 13%          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ By Severity:                            â”‚
â”‚ Critical: 1 (fixed) | High: 5 (2 open)  â”‚
â”‚ Medium: 8 (all fixed) | Low: 4 (1 open) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AUTOMATION METRICS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Total Test Cases     â”‚ 300              â”‚
â”‚ Automated            â”‚ 195 (65%)        â”‚
â”‚ Sprint New Auto      â”‚ +15              â”‚
â”‚ Automation Pass Rate â”‚ 98%              â”‚
â”‚ Execution Time       â”‚ 45 min           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SPRINT VELOCITY:
Planned: 45 points | Completed: 42 points
Velocity: 42 (vs 40 last sprint â†’ +5%)

RECOMMENDATIONS:
1. Address 2 blocked test cases (env issue)
2. Fix 2 open High severity bugs before release
3. Increase automation to 70% next sprint`
        },
        agileJiraExample: {
          context: "Configure Jira dashboards to auto-generate test metrics.",
          realWorldScenario: "QA dashboard shows real-time pass rate, defect trends, and sprint burndown.",
          sampleDocument: `JIRA QA DASHBOARD CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DASHBOARD: QA Sprint Metrics

GADGETS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TEST EXECUTION PIE CHART             â”‚
â”‚    Filter: type = "Test Case"           â”‚
â”‚            AND Sprint = activeSprint()  â”‚
â”‚    Group by: Execution Status           â”‚
â”‚                                         â”‚
â”‚    Display:                             â”‚
â”‚    ğŸŸ¢ Passed: 105 (91%)                 â”‚
â”‚    ğŸ”´ Failed: 8 (7%)                    â”‚
â”‚    ğŸŸ¡ Blocked: 2 (2%)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. DEFECT SEVERITY DISTRIBUTION         â”‚
â”‚    Filter: type = Bug                   â”‚
â”‚            AND Sprint = activeSprint()  â”‚
â”‚    Group by: Severity                   â”‚
â”‚                                         â”‚
â”‚    Display:                             â”‚
â”‚    ğŸ”´ Critical: â–ˆâ–ˆ 1                    â”‚
â”‚    ğŸŸ  High: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 5                     â”‚
â”‚    ğŸŸ¡ Medium: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8                â”‚
â”‚    ğŸŸ¢ Low: â–ˆâ–ˆâ–ˆâ–ˆ 4                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. BURNDOWN CHART                       â”‚
â”‚    Y: Story Points Remaining            â”‚
â”‚    X: Sprint Days                       â”‚
â”‚                                         â”‚
â”‚    Shows: Actual vs Ideal burndown      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. DEFECT TREND (Created vs Resolved)   â”‚
â”‚    Period: Last 5 sprints               â”‚
â”‚                                         â”‚
â”‚    Sprint 11: 15/12 (Gap: +3)           â”‚
â”‚    Sprint 12: 18/20 (Gap: -2)           â”‚
â”‚    Sprint 13: 14/15 (Gap: -1)           â”‚
â”‚    Sprint 14: 16/14 (Gap: +2)           â”‚
â”‚    Sprint 15: 18/15 (Gap: +3) âš ï¸        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`
        },
        tips: [
          "Focus on actionable metrics, not vanity metrics",
          "Always compare against targets/trends",
          "Know how to calculate each metric"
        ],
        commonMistakes: [
          "Tracking too many metrics",
          "Not acting on metric insights",
          "Missing trend analysis"
        ],
        followUpQuestions: [
          "What's more important: pass rate or coverage?",
          "How do you handle metric gaming?",
          "What metrics indicate team health?"
        ],
        difficulty: "Intermediate"
      }
    ]
  }
];

// Quick reference for common interview questions
export const quickReferenceQuestions = [
  "What is STLC?",
  "Difference between Smoke and Regression testing?",
  "How do you write a test case?",
  "What is BVA and EP?",
  "Explain Agile ceremonies",
  "What is Definition of Done?",
  "How do you prioritize test cases?",
  "What metrics do you track?",
  "How do you handle flaky tests?",
  "What is API testing?"
];

export const interviewTips = {
  preparation: [
    "Review your project experience and prepare specific examples",
    "Practice STAR method for behavioral questions",
    "Know the tools and technologies on your resume",
    "Prepare questions to ask the interviewer"
  ],
  during: [
    "Listen carefully to the full question before answering",
    "Ask for clarification if needed",
    "Use specific examples from your experience",
    "Be honest about what you don't know"
  ],
  common: [
    "Keep answers concise - 2-3 minutes max",
    "Quantify your achievements when possible",
    "Show enthusiasm for testing and quality",
    "Relate answers to the company's domain/products"
  ]
};
